{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from box import Box\n",
    "import yaml\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from model_logreg_mvn import ModelLogisicRegressionMvn\n",
    "from dataset_npz import DataModuleFromNPZ\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4124f52",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(2202)\n",
    "dm = DataModuleFromNPZ(\n",
    "    data_dir=\"data_logistic_regression_2d\",\n",
    "    feature_labels=[\"inputs\", \"targets\"],\n",
    "    batch_size=256,\n",
    "    num_workers=4,\n",
    "    shuffle_training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d37e1",
   "metadata": {},
   "source": [
    "# Train or load a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True: # chose True to train and save a model within this notebook, switch back to False to evaluate\n",
    "    dm.prepare_data()\n",
    "    dm.setup(stage=\"fit\")\n",
    "    model_mvn = ModelLogisicRegressionMvn(\n",
    "            2,\n",
    "            dm.size_train(),\n",
    "            scale_prior=10.0,\n",
    "            optimizer_name=\"RMSprop\", \n",
    "            optimizer_lr=0.1,\n",
    "            save_path=\"runs/models/multivariate\")\n",
    "    trainer = Trainer(max_epochs=100)\n",
    "    trainer.fit(model_mvn, dm)\n",
    "    trainer.test(model_mvn, dm)\n",
    "    model_mvn.eval()\n",
    "        \n",
    "else:\n",
    "    dm.prepare_data()\n",
    "    dm.setup(stage=\"fit\")\n",
    "    size_data_train = dm.size_train()\n",
    "    \n",
    "    SAVE_PATH = \"runs/models/multivariate/loss_val-epoch=160-step=644.ckpt\" # change this to your saved model in the same directory!!!\n",
    "    model_mvn = ModelLogisicRegressionMvn.load_from_checkpoint(SAVE_PATH, size_data=size_data_train)\n",
    "    dm.setup(stage=\"test\")\n",
    "    trainer = Trainer()\n",
    "    trainer.test(model_mvn, dm)\n",
    "    model_mvn.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3585e",
   "metadata": {},
   "source": [
    "# Load all training and testing data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35056ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_plotting = DataModuleFromNPZ(\n",
    "    data_dir=\"data_logistic_regression_2d\",\n",
    "    feature_labels=[\"inputs\", \"targets\"],\n",
    "    batch_size=-1,\n",
    "    num_workers=4,\n",
    "    shuffle_training=False\n",
    ")\n",
    "dm_plotting.prepare_data()\n",
    "dm_plotting.setup(stage=\"fit\")\n",
    "for f,l in dm_plotting.train_dataloader():\n",
    "    features_train, labels_train = f, l\n",
    "dm_plotting.setup(stage=\"test\")\n",
    "for f,l in dm_plotting.test_dataloader():\n",
    "    features_test, labels_test = f, l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076245c",
   "metadata": {},
   "source": [
    "# Compute class probabilities for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(np.arange(-1.1,1.1,0.025), np.arange(-1.1,1.1,0.025))\n",
    "features_plot = np.concatenate([x.reshape((-1,1)), y.reshape((-1,1))], axis=-1)\n",
    "p_plot_mvn  = model_mvn(torch.tensor(features_plot, dtype=torch.float32)).detach().cpu().numpy().reshape(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a9c1d",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "\n",
    "Ip = np.argwhere(labels_train[:] > 0.5)\n",
    "In = np.argwhere(labels_train[:] < 0.5)\n",
    "ax[0].contourf(x, y, p_plot_mvn, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "ax[0].plot(features_train[Ip,0], features_train[Ip,1], \".\", color = \"red\")\n",
    "ax[0].plot(features_train[In,0], features_train[In,1], \".\", color = \"blue\")\n",
    "ax[0].set_title(\"Multivariate model: Train data\")\n",
    "\n",
    "Ip = np.argwhere(labels_test[:] > 0.5)\n",
    "In = np.argwhere(labels_test[:] < 0.5)\n",
    "ax[1].contourf(x, y, p_plot_mvn, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "ax[1].plot(features_test[Ip,0], features_test[Ip,1], \".\", color = \"red\")\n",
    "ax[1].plot(features_test[In,0], features_test[In,1], \".\", color = \"blue\")\n",
    "ax[1].set_title(\"Multivariate model: Test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8fbe2c",
   "metadata": {},
   "source": [
    "# Print model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learned distribution parameters\")\n",
    "print(\"weights mean\")\n",
    "print(model_mvn.weights_loc.detach().cpu().numpy())\n",
    "print(\"weights covariance\")\n",
    "L = model_mvn.weights_chol().detach().cpu().numpy()\n",
    "print(np.matmul(L,L.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436edb25",
   "metadata": {},
   "source": [
    "# Plot exact Bayesian posterior distribution of the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_sigma(label_sign, features, w):\n",
    "    z = label_sign*np.sum(features.reshape((1,-1))*w, axis=-1)\n",
    "    #return np.abs(z) - np.log(1+np.exp(-z-np.abs(z)))\n",
    "    return -np.log(1 + np.exp(-z))\n",
    "    \n",
    "w1, w2 = np.meshgrid(np.linspace(-3*model_mvn.scale_prior,3*model_mvn.scale_prior, 101),\n",
    "                     np.linspace(-3*model_mvn.scale_prior,3*model_mvn.scale_prior, 101))\n",
    "shape_mesh = w1.shape\n",
    "\n",
    "w_plot = np.concatenate([w1.reshape((-1,1)), w2.reshape((-1,1))], axis=-1)\n",
    "log_prob_prior = -0.5*np.sum((w_plot)**2, axis=-1)/(model_mvn.scale_prior**2)\n",
    "log_prob_prior = log_prob_prior - 0.5*2.0*np.log(2*np.pi) - 0.5*2.0*2.0*np.log(model_mvn.scale_prior)\n",
    "\n",
    "#fig, ax = plt.subplots(1,3, figsize=(20,10))\n",
    "log_prob_current = log_prob_prior.reshape(shape_mesh)\n",
    "for i, (feature, label) in enumerate(zip(features_train.cpu().numpy(), labels_train.cpu().numpy())):\n",
    "    log_prob_likelihood = log_prob_sigma(2*label-1,feature, w_plot).reshape(shape_mesh)\n",
    "    \n",
    "    if i < 10:\n",
    "        plot_on = True\n",
    "    elif i % 200 == 0:\n",
    "        plot_on = True\n",
    "    else:\n",
    "        plot_on = False\n",
    "    \n",
    "    if plot_on:\n",
    "        fig, ax = plt.subplots(1,3, figsize=(24,8))\n",
    "        ax[0].contourf(w1, w2, log_prob_current, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "        ax[1].contourf(w1, w2, log_prob_likelihood, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "        ax[2].contourf(w1, w2, log_prob_current + log_prob_likelihood, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "        ax[0].set_title(f\"size_data: {i}\")\n",
    "        ax[1].set_title(\"New Likelihood\")\n",
    "        ax[2].set_title(\"New posterior\")\n",
    "    \n",
    "    log_prob_current = log_prob_current +  log_prob_likelihood \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187e4de",
   "metadata": {},
   "source": [
    "# Plot true Bayesin posterior distribuion vs approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "w_loc = model_mvn.weights_loc.detach().cpu().numpy().reshape((1,-1))\n",
    "L     = model_mvn.weights_chol().detach().cpu().numpy()\n",
    "\n",
    "log_prob_approx_mvn = -0.5*np.sum(np.matmul(w_plot-w_loc, np.linalg.inv(L).T)**2, axis=-1)\n",
    "log_prob_approx_mvn = log_prob_approx_mvn - 0.5*2.0*np.log(2*np.pi) - 0.5*2.0*np.sum(np.log(np.diag(L)))\n",
    "\n",
    "\n",
    "if True:\n",
    "    dw = (w1[0,1] - w1[0,0])*(w2[1,0] - w2[0,0])\n",
    "    log_prob_approx_mvn  = log_prob_approx_mvn - sp.special.logsumexp(log_prob_approx_mvn + np.log(dw))\n",
    "    log_prob_current     = log_prob_current - sp.special.logsumexp(log_prob_current + np.log(dw))\n",
    "    \n",
    " \n",
    "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "ax[0].contourf(w1, w2, log_prob_current, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "ax[0].set_title(\"Bayesian postrior\")\n",
    "ax[1].contourf(w1, w2, log_prob_approx_mvn.reshape(w1.shape), 50, cmap=plt.get_cmap(\"gray\"))\n",
    "ax[1].set_title(\"Mulrivariate approximation\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45095534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

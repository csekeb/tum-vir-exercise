{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5176dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from box import Box\n",
    "import yaml\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from model_logreg_dn import ModelLogisicRegressionMvn\n",
    "from dataset_npz import DataModuleFromNPZ\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4124f52",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504c5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(2202)\n",
    "dm = DataModuleFromNPZ(\n",
    "    data_dir=\"data_logistic_two_moons\",\n",
    "    feature_labels=[\"inputs\", \"targets\"],\n",
    "    batch_size=256,\n",
    "    num_workers=4,\n",
    "    shuffle_training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956d37e1",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00e9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.prepare_data()\n",
    "dm.setup(stage=\"fit\")\n",
    "\n",
    "feature_map = nn.Sequential(nn.Linear(2,256), nn.LeakyReLU(), nn.Linear(256,2))\n",
    "# WARNING: this feature map only transforms in 2d, you can use a d-dimensional outpout and \n",
    "# modify the regression model to work on d-dimensions fro better performance\n",
    "\n",
    "model_mvn = ModelLogisicRegressionMvn(\n",
    "        2,\n",
    "        dm.size_train(),\n",
    "        feature_map=feature_map,\n",
    "        is_diagonal=False,\n",
    "        scale_prior=10.0,\n",
    "        optimizer_name=\"RMSprop\", \n",
    "        optimizer_lr=0.1,\n",
    "        save_path=\"runs/models/multivariate\")\n",
    "trainer = Trainer(max_epochs=50)\n",
    "trainer.fit(model_mvn, dm)\n",
    "trainer.test(model_mvn, dm)\n",
    "model_mvn.eval()\n",
    "\n",
    "model_diag = ModelLogisicRegressionMvn(\n",
    "        2,\n",
    "        dm.size_train(),\n",
    "        feature_map=feature_map,\n",
    "        is_diagonal=True,\n",
    "        scale_prior=10.0,\n",
    "        optimizer_name=\"RMSprop\", \n",
    "        optimizer_lr=0.1,\n",
    "        save_path=\"runs/models/diagonal\")\n",
    "trainer = Trainer(max_epochs=50)\n",
    "trainer.fit(model_diag, dm)\n",
    "trainer.test(model_diag, dm)\n",
    "model_diag.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c3585e",
   "metadata": {},
   "source": [
    "# Load all training and testing data for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35056ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_plotting = DataModuleFromNPZ(\n",
    "    data_dir=\"data_logistic_two_moons\",\n",
    "    feature_labels=[\"inputs\", \"targets\"],\n",
    "    batch_size=-1,\n",
    "    num_workers=4,\n",
    "    shuffle_training=False\n",
    ")\n",
    "dm_plotting.prepare_data()\n",
    "dm_plotting.setup(stage=\"fit\")\n",
    "for f,l in dm_plotting.train_dataloader():\n",
    "    features_train, labels_train = f, l\n",
    "dm_plotting.setup(stage=\"test\")\n",
    "for f,l in dm_plotting.test_dataloader():\n",
    "    features_test, labels_test = f, l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e076245c",
   "metadata": {},
   "source": [
    "# Compute class probabilities for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.meshgrid(np.arange(-3,3,0.025), np.arange(-3, 3, 0.025))\n",
    "features_plot = np.concatenate([x.reshape((-1,1)), y.reshape((-1,1))], axis=-1)\n",
    "p_plot_mvn  = model_mvn(torch.tensor(features_plot, dtype=torch.float32)).detach().cpu().numpy().reshape(x.shape)\n",
    "p_plot_diag = model_diag(torch.tensor(features_plot, dtype=torch.float32)).detach().cpu().numpy().reshape(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a9c1d",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f4e63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "\n",
    "Ip = np.argwhere(labels_train[:] > 0.5)\n",
    "In = np.argwhere(labels_train[:] < 0.5)\n",
    "ax[0].contourf(x, y, p_plot_mvn, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "ax[0].plot(features_train[Ip,0], features_train[Ip,1], \".\", color = \"red\")\n",
    "ax[0].plot(features_train[In,0], features_train[In,1], \".\", color = \"blue\")\n",
    "ax[0].set_title(\"Multivariate model: Train data\")\n",
    "\n",
    "Ip = np.argwhere(labels_test[:] > 0.5)\n",
    "In = np.argwhere(labels_test[:] < 0.5)\n",
    "ax[1].contourf(x, y, p_plot_mvn, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "ax[1].plot(features_test[Ip,0], features_test[Ip,1], \".\", color = \"red\")\n",
    "ax[1].plot(features_test[In,0], features_test[In,1], \".\", color = \"blue\")\n",
    "ax[1].set_title(\"Multivariate model: Test data\")\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(20,10))\n",
    "\n",
    "Ip = np.argwhere(labels_train[:] > 0.5)\n",
    "In = np.argwhere(labels_train[:] < 0.5)\n",
    "ax[0].contourf(x, y, p_plot_diag, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "ax[0].plot(features_train[Ip,0], features_train[Ip,1], \".\", color = \"red\")\n",
    "ax[0].plot(features_train[In,0], features_train[In,1], \".\", color = \"blue\")\n",
    "ax[0].set_title(\"Diagonal model: Train data\")\n",
    "\n",
    "Ip = np.argwhere(labels_test[:] > 0.5)\n",
    "In = np.argwhere(labels_test[:] < 0.5)\n",
    "ax[1].contourf(x, y, p_plot_diag, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "ax[1].plot(features_test[Ip,0], features_test[Ip,1], \".\", color = \"red\")\n",
    "ax[1].plot(features_test[In,0], features_test[In,1], \".\", color = \"blue\")\n",
    "ax[1].set_title(\"Diagonal model: Test data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8fbe2c",
   "metadata": {},
   "source": [
    "# Print model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de5078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Learned distribution parameters\")\n",
    "print(\"weights mean\")\n",
    "print(model_mvn.weights_loc.detach().cpu().numpy())\n",
    "print(\"weights covariance\")\n",
    "L = model_mvn.weights_chol().detach().cpu().numpy()\n",
    "print(np.matmul(L,L.T))\n",
    "\n",
    "\n",
    "print(\"Learned distribution parameters\")\n",
    "print(\"weights mean\")\n",
    "print(model_diag.weights_loc.detach().cpu().numpy())\n",
    "print(\"weights covariance\")\n",
    "L = model_diag.weights_chol().detach().cpu().numpy()\n",
    "print(np.matmul(L,L.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6722050e",
   "metadata": {},
   "source": [
    "# Plot Bayesian  posterior distribution of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0046ec92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_sigma(label_sign, features, w):\n",
    "    z = label_sign*np.sum(features.reshape((1,-1))*w, axis=-1)\n",
    "    #return np.abs(z) - np.log(1+np.exp(-z-np.abs(z)))\n",
    "    return -np.log(1 + np.exp(-z))\n",
    "    \n",
    "w1, w2 = np.meshgrid(np.linspace(-3*model_diag.scale_prior,3*model_diag.scale_prior, 101),\n",
    "                     np.linspace(-3*model_diag.scale_prior,3*model_diag.scale_prior, 101))\n",
    "shape_mesh = w1.shape\n",
    "\n",
    "w_plot = np.concatenate([w1.reshape((-1,1)), w2.reshape((-1,1))], axis=-1)\n",
    "log_prob_prior = -0.5*np.sum((w_plot)**2, axis=-1)/(model_diag.scale_prior**2)\n",
    "log_prob_prior = log_prob_prior - 0.5*2.0*np.log(2*np.pi) - 0.5*2.0*2.0*np.log(model_diag.scale_prior)\n",
    "\n",
    "#fig, ax = plt.subplots(1,3, figsize=(20,10))\n",
    "log_prob_current = log_prob_prior.reshape(shape_mesh)\n",
    "for i, (feature, label) in enumerate(zip(features_train.cpu().numpy(), labels_train.cpu().numpy())):\n",
    "    log_prob_likelihood = log_prob_sigma(2*label-1,feature, w_plot).reshape(shape_mesh)\n",
    "    \n",
    "    if i < 10:\n",
    "        plot_on = True\n",
    "    elif i % 100 == 0:\n",
    "        plot_on = True\n",
    "    else:\n",
    "        plot_on = False\n",
    "    \n",
    "    if plot_on:\n",
    "        fig, ax = plt.subplots(1,3, figsize=(24,8))\n",
    "        ax[0].contourf(w1, w2, log_prob_current, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "        ax[1].contourf(w1, w2, log_prob_likelihood, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "        ax[2].contourf(w1, w2, log_prob_current + log_prob_likelihood, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "        ax[0].set_title(f\"size_data: {i}\")\n",
    "    \n",
    "    log_prob_current = log_prob_current +  log_prob_likelihood \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187e4de",
   "metadata": {},
   "source": [
    "# Plot posterior distribuion vs approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dcab38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "\n",
    "w_loc = model_mvn.weights_loc.detach().cpu().numpy().reshape((1,-1))\n",
    "L     = model_mvn.weights_chol().detach().cpu().numpy()\n",
    "\n",
    "log_prob_approx_mvn = -0.5*np.sum(np.matmul(w_plot-w_loc, np.linalg.inv(L).T)**2, axis=-1)\n",
    "log_prob_approx_mvn = log_prob_approx_mvn - 0.5*2.0*np.log(2*np.pi) - 0.5*2.0*np.sum(np.log(np.diag(L)))\n",
    "\n",
    "\n",
    "w_loc = model_diag.weights_loc.detach().cpu().numpy().reshape((1,-1))\n",
    "L_diag = torch.exp(model_diag.weights_scale_logdiag).detach().cpu().numpy().reshape((1,-1))\n",
    "\n",
    "log_prob_approx_diag = -0.5*np.sum(((w_plot-w_loc)/L_diag)**2, axis=-1)\n",
    "log_prob_approx_diag = log_prob_approx_diag - 0.5*2.0*np.log(2*np.pi) - 0.5*2.0*np.sum(np.log(L_diag))\n",
    "\n",
    "if True:\n",
    "    dw = (w1[0,1] - w1[0,0])*(w2[1,0] - w2[0,0])\n",
    "    log_prob_approx_mvn  = log_prob_approx_mvn - sp.special.logsumexp(log_prob_approx_mvn + np.log(dw))\n",
    "    log_prob_approx_diag = log_prob_approx_diag - sp.special.logsumexp(log_prob_approx_diag + np.log(dw))\n",
    "    log_prob_current     = log_prob_current - sp.special.logsumexp(log_prob_current + np.log(dw))\n",
    "    \n",
    " \n",
    "fig, ax = plt.subplots(2,2, figsize=(20,10))\n",
    "ax[0,0].contourf(w1, w2, log_prob_current, 50, cmap=plt.get_cmap(\"gray\"))\n",
    "ax[0,0].set_title(\"Bayesian postrior\")\n",
    "ax[0,1].contourf(w1, w2, log_prob_approx_mvn.reshape(w1.shape), 50, cmap=plt.get_cmap(\"gray\"))\n",
    "ax[0,1].set_title(\"Mulrivariate approximation\")\n",
    "ax[1,1].contourf(w1, w2, log_prob_approx_diag.reshape(w1.shape), 50, cmap=plt.get_cmap(\"gray\")) \n",
    "ax[1,1].set_title(\"Diagonal approximation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4018e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f310d56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
